---
title: "Assignment 4"
output: Sharmili Tandulwadkar
---

```{r}

library(ISLR)
library(readr)
library(tidyverse)  # data manipulation
library(cluster)    # clustering algorithms
library(factoextra) # clustering visualization
library(dendextend) # for comparing two dendrograms

customer <- read.csv("Cereals.csv")
View(customer)
set.seed(123)

#Remove the categorical variables
df <- customer[,-c(1:3)]

#df <- cbind(customer[,1:3],df)

#Remove NA (missing) values
df <- na.omit(df)

#Data Normalization
df <- scale(df)
View(df)
summary(df)

```




```{r}
# Hierarchical clustering to data using using Euclidean distance

#Dissimilarity matrix
d <- dist(df, method = "euclidean")

# Hierarchical clustering using Complete Linkage
hc_complete <- hclust(d, method = "complete" )

# Plot the obtained dendrogram
plot(hc_complete, cex = 0.6, hang = -1)
```



```{r}
# Compute with agnes and with different linkage methods

m <- c( "average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")

# function to compute coefficient
ac <- function(x) {
  agnes(df, method = x)$ac
}

map_dbl(m, ac)
```

Using Ward's method we are getting the highest agglomerative coefficient. Ward method is the best linkage method with agglomerative coefficient  0.9046042. 

```{r}

#Visualizing the dendogram using ward's method

hc2 <- agnes(df, method = "ward")
pltree(hc2, cex = 0.6, hang = -1, main = "Dendrogram of agnes")

```


```{r}
#For identifying clusters we can cut the dendrogram with cutree()
#Create the distance matrix
d <- dist(df, method = "euclidean")

# Hierarchical clustering using Ward Linkage
ward_cluster <- hclust(d, method = "ward.D2" )

# Plot the obtained dendrogram
plot(ward_cluster, cex = 0.6, hang = -1)

sub_grp <- cutree(ward_cluster, k = 4)
plot(ward_cluster, cex = 0.6)
rect.hclust(ward_cluster, k = 4, border = 2:5)
fviz_cluster(list(data = df, cluster = sub_grp))

```


```{r}
#The number records of the data assigned to clusters 

# Cut tree into 6 groups
sub_grp <- cutree(hc_ward_cut, k = 6)

# Number of members in each cluster
table(sub_grp)

```
```{r}
#visualize the result in a plot

fviz_cluster(list(data = df, cluster = sub_grp))
```
```{r}
library(GGally)
library(dplyr)
cust %>% 
  select(calories, protein, fat, sodium, fiber, carbo, sugars, potass,vitamins,rating) %>% 
  ggcorr(palette = "RdBu", label = TRUE, label_round =  2)

```


This correlation matrix will help to understand whether a strong or weak relation existing between the variables. Clusters with stability value less than 0.6 should be considered unstable. Values between 0.6 to 0.75 indicates the pattern for data but isn't shows the which points should be clustered together. Clusters with stability values above 0.85 can be considered highly stable.
```{r}
#Running clusterboot()
library(fpc)
kbest.p<-6
cboot <- clusterboot(df,clustermethod=hclustCBI,method="ward.D2", k=kbest.p)

summary(cboot$result)
groups<-cboot$result$partition
head(data.frame(groups))

```
```{r}
#The vector of cluster stabilities
cboot$bootmean
```

```{r}
#The count of how many times each cluste was dissolved. By default clusterboot() runs 100 bootstrap iterations.
cboot$bootbrd
```

```{r}
groups <- cutree(hc_ward_cut, k = 6)
print_clusters <- function(labels, k) {
for(i in 1:k) {
print(paste("cluster", i))
print(cust[labels==i,c("mfr","calories","protein","fat","sodium","fiber","carbo","sugars","potass","vitamins","rating")])
}
}
print_clusters(groups, 6)
```

